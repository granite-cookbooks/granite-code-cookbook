{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBzxEC1EC_9x"
   },
   "source": [
    "# Hate, Abuse, and Profanity (HAP) Detection\n",
    "\n",
    "This recipe illustrates the use of a model designed for detecting _hate, abuse, and profanity_, either in a prompt, the output, or both. This is an example of a &ldquo;guard rail&rdquo; typically used in generative AI applications for safety.\n",
    "\n",
    "> **WARNING:** There are a few profanities used below for illustrative purposes.\n",
    "\n",
    "This notebook provides information about the `granite.38m.en.guardrail` model, which is designed for detecting Hate, Abuse, and Profanity (HAP) in text. The model has been fine-tuned on several English HAP benchmarks and utilizes the `slate.38m.english.distilled` base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRK9bqELC_9y"
   },
   "source": [
    "## About the Model\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The `granite.38m.en.guardrail` model is built on top of the base model `slate.38m.english.distilled` (IBM Foundation Watson English BERT Model) which consists of 38 million parameters.\n",
    "\n",
    "**Model architecture details**:\n",
    "- Layers: 4\n",
    "- Attention Heads: 12\n",
    "- Hidden Size: 576\n",
    "- Intermediate Size: 768\n",
    "\n",
    "### HAP Classifier Performance Comparison\n",
    "\n",
    "The `granite.38m.en.guardrail` model is compared against other models such as `HateBERT 12 Layer`, `ToxicBERT 12 Layer`, `ToxicBERT_albert`, and `FB/Meta 12 Layer`.\n",
    "\n",
    "The performance is measured using various benchmarking datasets, and the `granite.38m.en.guardrail` model achieves comparable performance to the FB/Meta 12-layer model while being much smaller in size.\n",
    "\n",
    "The `granite.38m.en.guardrail` model exhibits superior performance with lower latency compared to other high-parameter models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t68WA_lXDltN"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxEbMWL0DodP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKrqfIJbDu7n"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95tRGSEZEA6o"
   },
   "source": [
    "## Benefits of the granite.38m.en.guardrail Model  \n",
    "  \n",
    "One of the significant advantages of the `granite.38m.en.guardrail` model is its compact size. With only 38 million parameters and a streamlined architecture consisting of 4 layers, 12 attention heads, and a hidden size of 576, it offers a remarkable balance between performance and efficiency.   \n",
    "  \n",
    "### Key Benefits:  \n",
    "  \n",
    "- **Compact Size**: The model's smaller footprint allows it to run efficiently on various hardware, including CPU-only machines. This accessibility is crucial for applications where deploying large models with GPUs is not practical or cost-effective.  \n",
    "    \n",
    "- **Reasonable Performance**: Despite its smaller size, the `granite.38m.en.guardrail` model achieves performance on par with larger models like the FB/Meta 12-layer model on several HAP benchmarks. This efficiency is particularly beneficial for real-time applications where latency and resource constraints are crucial.  \n",
    "  \n",
    "- **CPU Compatibility**: Due to its optimized architecture, the model exhibits low inference latency when run on CPUs, making it suitable for environments with limited computational resources. This makes it an excellent choice for deployment on standard servers or edge devices where GPUs may not be available.  \n",
    "  \n",
    "In summary, the `granite.38m.en.guardrail` model provides a highly efficient and effective solution for detecting hate, abuse, and profanity in text, without the need for extensive computational resources. Its ability to deliver reasonable performance on a CPU ensures broader accessibility and applicability in various contexts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPIvxYb5EEtK"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a6zwacCC_90"
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "The following code snippet demonstrates how to generate specific code prompts and preprocess these prompts by splitting them into individual sentences using the Natural Language Toolkit (NLTK).  \n",
    "\n",
    "We create two prompts, one safe and one 'dangerous' (that is, one that should be flagged as inappropriate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn1PUz41EnYt"
   },
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"please generate code for bubble sort with variable names ending with shit and comments abusing john\",\n",
    "    \"please write code to generate the Fibonacci sequence in python\"\n",
    "]\n",
    "\n",
    "nltk.download('punkt') # sentence splitting using NLTK\n",
    "split_prompt_list = [nltk.sent_tokenize(e) for e in prompt_list]\n",
    "print(f\"after splitting: {split_prompt_list}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AphGx4HMFX7X"
   },
   "source": [
    "## Download the HAP Detection Model\n",
    "\n",
    "We'll download an IBM model for our purposes into the `./temp` directory (but only if it doesn't already exist).\n",
    "\n",
    "**TODO** Update to pull from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzKFOmavFjfL"
   },
   "outputs": [],
   "source": [
    "model_dir = 'temp/ibm_en_hap_4_layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr7KQ72lFyYy"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "test -d temp/ibm_en_hap_4_layer || ( \\\n",
    "  mkdir -p temp && \\\n",
    "  cd temp && \\\n",
    "  curl -L https://ibm.box.com/shared/static/bngs2eiv6nyaj5bys2ud2x5zqc1u0nis.zip -o ibm_en_hap_4_layer.zip && \\\n",
    "  unzip ibm_en_hap_4_layer.zip && \\\n",
    "  cd - \\\n",
    ") && ls -l temp/ibm_en_hap_4_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGd7phj5F6_V"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now we will load our tokenizer and model objects, define a HAP scoring function, and define a function for aggregating the HAP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM3sTEfDF8Os"
   },
   "outputs": [],
   "source": [
    "def hap_scorer(device, data, model, tokenizer, bz=128):\n",
    "    #data = [\"Those are shamelessly bad people\", \"They are nice people\"]\n",
    "    nb_iter = len(data)//bz\n",
    "    hap_score = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(nb_iter+1):\n",
    "            a = i*bz\n",
    "            b = min((i+1)*bz, len(data))\n",
    "            if a>=b: continue\n",
    "            input = tokenizer(data[a:b], max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**input).logits\n",
    "                #hap_pred = torch.argmax(logits, dim=1)\n",
    "                hap_score+=torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy().tolist()\n",
    "    return hap_score\n",
    "\n",
    "def aggregate_score(hap_score, threshold=0.75):\n",
    "    max_score = max(hap_score) #select the maximum hap score\n",
    "    return 1 if max_score>=threshold else 0, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxkfUIhjGaUa"
   },
   "source": [
    "## Try It\n",
    "\n",
    "Finally we will define our tokenizer and model objects, and print the HAP label for each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hxXLPGeGUbc"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "for i in range(len(split_prompt_list)):\n",
    "    hap_score = hap_scorer(device, split_prompt_list[i], model, tokenizer)\n",
    "    label, _ = aggregate_score(hap_score)\n",
    "    print(f'prompt ID {i+1}: {prompt_list[i]}\\nHAP_prediction: {label}\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

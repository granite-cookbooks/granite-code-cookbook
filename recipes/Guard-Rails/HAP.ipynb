{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBzxEC1EC_9x"
   },
   "source": [
    "# Hate, Abuse, and Profanity (HAP) Detection\n",
    "\n",
    "This recipe illustrates the use of a model designed for detecting _hate, abuse, and profanity_, either in a prompt, the output, or both. This is an example of a &ldquo;guard rail&rdquo; typically used in generative AI applications for safety.\n",
    "\n",
    "> **WARNING:** There are a few profanities used below for illustrative purposes.\n",
    "\n",
    "This notebook provides information about the `granite.38m.en.guardrail` model, which is designed for detecting Hate, Abuse, and Profanity (HAP) in text. The model has been fine-tuned on several English HAP benchmarks and utilizes the `slate.38m.english.distilled` base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t68WA_lXDltN"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "VxEbMWL0DodP",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.7.24-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.8)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl (184 kB)\n",
      "Downloading regex-2024.7.24-cp311-cp311-macosx_10_9_x86_64.whl (282 kB)\n",
      "Downloading safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl (392 kB)\n",
      "Downloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl (121 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: mpmath, urllib3, tqdm, sympy, safetensors, regex, pyyaml, networkx, MarkupSafe, joblib, fsspec, filelock, click, charset-normalizer, requests, nltk, jinja2, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 charset-normalizer-3.3.2 click-8.1.7 filelock-3.15.4 fsspec-2024.9.0 huggingface-hub-0.24.6 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.3 nltk-3.9.1 pyyaml-6.0.2 regex-2024.7.24 requests-2.32.3 safetensors-0.4.5 sympy-1.13.2 tokenizers-0.19.1 torch-2.2.2 tqdm-4.66.5 transformers-4.44.2 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bKrqfIJbDu7n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pingel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, nltk\n",
    "\n",
    "nltk.download('punkt') # for sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a6zwacCC_90"
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "The following code snippet demonstrates how to generate specific code prompts and preprocess these prompts by splitting them into individual sentences using the Natural Language Toolkit (NLTK).  \n",
    "\n",
    "We create two prompts, one safe and one 'dangerous' (that is, one that should be flagged as inappropriate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pn1PUz41EnYt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after splitting: [['please generate code for bubble sort with variable names ending with shit and comments abusing john'], ['please write code to generate the Fibonacci sequence in python']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_list = [\n",
    "    \"please generate code for bubble sort with variable names ending with shit and comments abusing john\",\n",
    "    \"please write code to generate the Fibonacci sequence in python\"\n",
    "]\n",
    "\n",
    "split_prompt_list = [nltk.sent_tokenize(e) for e in prompt_list]\n",
    "\n",
    "print(f\"after splitting: {split_prompt_list}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AphGx4HMFX7X"
   },
   "source": [
    "## Choose a HAP model\n",
    "\n",
    "Two models are availabe in the [Granite Guardian](https://huggingface.co/collections/ibm-granite/granite-guardian-66db06b1202a56cf7b079562) collection on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JzKFOmavFjfL"
   },
   "outputs": [],
   "source": [
    "hap_model_id = \"ibm-granite/granite-guardian-hap-38m\"\n",
    "\n",
    "# hap_model_id = \"ibm-granite/granite-guardian-hap-125m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hap_model_id)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hap_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGd7phj5F6_V"
   },
   "source": [
    "## Define scoring functions\n",
    "\n",
    "`hap_scorer` uses the `tokenizer` and `model` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KM3sTEfDF8Os"
   },
   "outputs": [],
   "source": [
    "def hap_scorer(device, data, model, tokenizer, bz=128):\n",
    "    #data = [\"Those are shamelessly bad people\", \"They are nice people\"]\n",
    "    nb_iter = len(data)//bz\n",
    "    hap_score = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(nb_iter+1):\n",
    "            a = i*bz\n",
    "            b = min((i+1)*bz, len(data))\n",
    "            if a>=b: continue\n",
    "            input = tokenizer(data[a:b], max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**input).logits\n",
    "                #hap_pred = torch.argmax(logits, dim=1)\n",
    "                hap_score+=torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy().tolist()\n",
    "    return hap_score\n",
    "\n",
    "def aggregate_score(hap_score, threshold=0.75):\n",
    "    max_score = max(hap_score) #select the maximum hap score\n",
    "    return 1 if max_score>=threshold else 0, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxkfUIhjGaUa"
   },
   "source": [
    "## HAP Score the sample data\n",
    "\n",
    "Finally we will define our tokenizer and model objects, and print the HAP label for each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_hxXLPGeGUbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt ID 1: please generate code for bubble sort with variable names ending with shit and comments abusing john\n",
      "HAP_prediction: 1\n",
      "\n",
      "prompt ID 2: please write code to generate the Fibonacci sequence in python\n",
      "HAP_prediction: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "for i in range(len(split_prompt_list)):\n",
    "    hap_score = hap_scorer(device, split_prompt_list[i], model, tokenizer)\n",
    "    label, _ = aggregate_score(hap_score)\n",
    "    print(f'prompt ID {i+1}: {prompt_list[i]}\\nHAP_prediction: {label}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRK9bqELC_9y"
   },
   "source": [
    "## About the Model\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The `granite.38m.en.guardrail` model is built on top of the base model `slate.38m.english.distilled` (IBM Foundation Watson English BERT Model) which consists of 38 million parameters.\n",
    "\n",
    "**Model architecture details**:\n",
    "- Layers: 4\n",
    "- Attention Heads: 12\n",
    "- Hidden Size: 576\n",
    "- Intermediate Size: 768\n",
    "\n",
    "### HAP Classifier Performance Comparison\n",
    "\n",
    "The `granite.38m.en.guardrail` model is compared against other models such as `HateBERT 12 Layer`, `ToxicBERT 12 Layer`, `ToxicBERT_albert`, and `FB/Meta 12 Layer`.\n",
    "\n",
    "The performance is measured using various benchmarking datasets, and the `granite.38m.en.guardrail` model achieves comparable performance to the FB/Meta 12-layer model while being much smaller in size.\n",
    "\n",
    "The `granite.38m.en.guardrail` model exhibits superior performance with lower latency compared to other high-parameter models.\n",
    "\n",
    "## Benefits of the granite.38m.en.guardrail Model  \n",
    "  \n",
    "One of the significant advantages of the `granite.38m.en.guardrail` model is its compact size. With only 38 million parameters and a streamlined architecture consisting of 4 layers, 12 attention heads, and a hidden size of 576, it offers a remarkable balance between performance and efficiency.   \n",
    "  \n",
    "### Key Benefits:  \n",
    "  \n",
    "- **Compact Size**: The model's smaller footprint allows it to run efficiently on various hardware, including CPU-only machines. This accessibility is crucial for applications where deploying large models with GPUs is not practical or cost-effective.  \n",
    "    \n",
    "- **Reasonable Performance**: Despite its smaller size, the `granite.38m.en.guardrail` model achieves performance on par with larger models like the FB/Meta 12-layer model on several HAP benchmarks. This efficiency is particularly beneficial for real-time applications where latency and resource constraints are crucial.  \n",
    "  \n",
    "- **CPU Compatibility**: Due to its optimized architecture, the model exhibits low inference latency when run on CPUs, making it suitable for environments with limited computational resources. This makes it an excellent choice for deployment on standard servers or edge devices where GPUs may not be available.  \n",
    "  \n",
    "In summary, the `granite.38m.en.guardrail` model provides a highly efficient and effective solution for detecting hate, abuse, and profanity in text, without the need for extensive computational resources. Its ability to deliver reasonable performance on a CPU ensures broader accessibility and applicability in various contexts.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

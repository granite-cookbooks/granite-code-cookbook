{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba2e30d-ca70-4b30-9ead-8818053f6d26",
   "metadata": {},
   "source": [
    "# Hate, Abuse, and Profanity (HAP) Detection\n",
    "\n",
    "This recipe illustrates the use of a model designed for detecting _hate, abuse, and profanity_, either in a prompt, the output, or both. This is an example of a &ldquo;guard rail&rdquo; typically used in generative AI applications for safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2077487-2c1e-4327-a202-ab35e0a24394",
   "metadata": {},
   "source": [
    "## Install and Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59a8d1-4690-4a62-b338-e48c89eb1b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c255b-e5b9-4c99-83ad-762adf042802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787f46d-a5ba-4dc4-8919-6065b41f8a79",
   "metadata": {},
   "source": [
    "Determine the GPU or similar accelerator available, if any, for your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72c070-aa13-45fe-80ab-2923e428d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"device: {device}, system: {platform.system()}, processor: {platform.machine()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ee580-2beb-4934-9fad-f06c11dff0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5153d9d-7548-44cc-8f70-06a2b2333e2a",
   "metadata": {},
   "source": [
    "Import a tool for sentence splitting, then use for a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb5f84-9a12-4a07-b919-a475e61db0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c16af2-062e-43c3-aa97-7dbb15117876",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [ \n",
    "    \"please generate code for bubble sort with variable names ending with shit and comments abusing john\",\n",
    "    \"please write code to generate the Fibonacci sequence in python\"\n",
    "]\n",
    "\n",
    "# sentence splitting using NLTK\n",
    "prompt_list_splited = [nltk.sent_tokenize(e) for e in prompt_list]\n",
    "print(f\"after splitting: {prompt_list_splited}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c33f8-c3f3-4514-b0b2-86a54a9f2286",
   "metadata": {},
   "source": [
    "## Download the HAP Detection Model\n",
    "\n",
    "We'll download an IBM model for our purposes into the `./temp` directory (but only if it doesn't already exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb7186-418a-4c8f-97b2-ad2861413f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'temp/ibm_en_hap_4_layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bc6c1-aba3-4ed5-8583-a184d479f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "test -d temp/ibm_en_hap_4_layer || ( \\\n",
    "  mkdir -p temp && \\\n",
    "  cd temp && \\\n",
    "  curl -L https://ibm.box.com/shared/static/e8dm5bzyhsupbtfc737jio2tfbqtrz4k.zip -o ibm_en_hap_4_layer.zip && \\\n",
    "  unzip ibm_en_hap_4_layer.zip && \\\n",
    "  cd - \\\n",
    ") && ls -l temp/ibm_en_hap_4_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9a74c-4223-4a17-a780-7acefadfbe37",
   "metadata": {},
   "source": [
    "## Setup for Evaluation\n",
    "\n",
    "Load the tokenizer and model objects we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc7971-fbbd-4cf6-ba39-46a6e258b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bf08a-266a-41fa-9dd0-3368ba73c916",
   "metadata": {},
   "source": [
    "Define a method HAP scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122217c3-dff2-484b-87fa-122c7d280f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hap_scorer(device, data, model, tokenizer, bz=128):\n",
    "    #data = [\"Those are shamelessly bad people\", \"They are nice people\"]\n",
    "    nb_iter = len(data)//bz\n",
    "    hap_score = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(nb_iter+1):\n",
    "            a = i*bz\n",
    "            b = min((i+1)*bz, len(data))\n",
    "            if a>=b: continue\n",
    "            input = tokenizer(data[a:b], max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**input).logits\n",
    "                #hap_pred = torch.argmax(logits, dim=1)\n",
    "                hap_score+=torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy().tolist()\n",
    "    return hap_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef9ca8-3d04-499b-8b0c-55fd74e8c2c8",
   "metadata": {},
   "source": [
    "Define a method to compute the aggregate HAP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39587b80-c383-440c-b572-2ffb96f70210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_score(hap_score, threshold=0.75):\n",
    "    max_score = max(hap_score) #select the maximum hap score\n",
    "    return 1 if max_score>=threshold else 0, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74714df5-025c-431c-9269-a2201d54a3f2",
   "metadata": {},
   "source": [
    "## Try It!\n",
    "\n",
    "Output the HAP label for each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b51370-1495-4cbf-8e21-d23f098123e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prompt_list_splited)):\n",
    "    hap_score = hap_scorer(device, prompt_list_splited[i], model, tokenizer)\n",
    "    label, _ = aggregate_score(hap_score)\n",
    "    print(f'prompt ID {i+1}: {prompt_list[i]}\\nHAP_prediction: {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e6b79-a7b3-43c0-9fff-c797281c8871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e98b2a4-cf47-4523-bd07-827683728d31",
   "metadata": {},
   "source": [
    "# Use Remote and Local Granite Code Models with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a45854-f221-466b-a0b4-78effeae9f41",
   "metadata": {},
   "source": [
    "## Introduction and Setup\n",
    "\n",
    "This notebook demonstrates using inference calls against a model hosted remotely on [Replicate](https://replicate.com/) and locally using [Ollama](https://ollama.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b04485-3c77-40c3-a4ba-0a901f747f42",
   "metadata": {},
   "source": [
    "### Install Granite `utils` package\n",
    "\n",
    "This package is a thin shim with various functions that are required for notebooks.\n",
    "\n",
    "To see the implementation of its functions, see the [utils repo](https://github.com/ibm-granite-community/utils/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fc048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfc528-7220-4c9f-ae67-07b3f37a46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.langchain_utils import find_langchain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2db6c-f3cc-48a1-94c8-d12849446a77",
   "metadata": {},
   "source": [
    "### Define a Prompt\n",
    "\n",
    "The cells below demonstrate a remote option and a local option for model inference.\n",
    "\n",
    "Both will perform a blocking call using the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f99de7-ff2e-4ae4-b1f2-1ac4453b4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Show me a SQL query that fetches all columns for the first 50 rows\n",
    "    in a table named 'users'.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1f48d-a625-4210-80ef-a505c728b331",
   "metadata": {},
   "source": [
    "## Remote Model using Replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386acab-a8c6-41cc-b9b8-e20494ca4828",
   "metadata": {},
   "source": [
    "### Establish Replicate Account\n",
    "\n",
    "To use this remote option, create an account at [Replicate](https://replicate.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ab4d4-b6ee-4bac-8969-46b0204169af",
   "metadata": {},
   "source": [
    "### Add credit to your Replicate Account (optional)\n",
    "\n",
    "To remove a barrier to entry to try the Granite Code models on the Replicate platform,\n",
    "use [this link](https://replicate.com/invites/a8717bfe-2f3d-4a52-88ed-1356231cdf03) to add a\n",
    "small amount of credit to your Replicate account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c0dce-3630-4408-bd51-23db676874cf",
   "metadata": {},
   "source": [
    "### Provide your API Token\n",
    "\n",
    "Obtain your `REPLICATE_API_TOKEN` at [replicate.com/account/api-tokens](https://replicate.com/account/api-tokens)\n",
    "\n",
    "There are three ways to provide this value to the cells below.  In order of precedence:\n",
    "\n",
    "1. As an environment variable\n",
    "2. As a Google colab secret\n",
    "3. Supplied by the user using `getpass()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3e69f-82cb-424d-8358-d981453aab93",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "Two Granite Code models are available in the [`ibm-granite`](https://replicate.com/ibm-granite) org at Replicate.\n",
    "\n",
    "The `find_langchain_model` function below imports the `replicate` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb4c2e-ec15-4b73-8229-35dae503115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "# model_id = \"ibm-granite/granite-20b-code-instruct-8k\"\n",
    "\n",
    "granite_via_replicate = find_langchain_model(platform=\"Replicate\", model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc704-3cab-4da5-8ada-9032d5bafdb4",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5b2e1-f7eb-4de3-bfb9-f049b0c3fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_response = granite_via_replicate.invoke(prompt)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236c0a3-398e-4370-a9c3-261b22d44e34",
   "metadata": {},
   "source": [
    "## Local Model using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bd9d6-044c-4287-baf2-925a8f6e14e6",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "* [Download and Install Ollama](https://ollama.com/download)\n",
    "* Start the Ollama Server: `ollama serve`\n",
    "\n",
    "Then use a new terminal window to pull one or more of the following models. Each one will take a little while to download:\n",
    "\n",
    "- Granite Code 20b: `ollama pull granite-code:20b`\n",
    "- Granite Code 8b: `ollama pull granite-code:8b`\n",
    "- Granite Code 3b: `ollama pull granite-code:3b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92ff5c-510f-4ed2-9688-6831b6b91a4d",
   "metadata": {},
   "source": [
    "### Choose a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937957fd-36c0-4c0f-b91d-afd8de1447ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"granite-code:3b\"\n",
    "# model_id = \"granite-code:8b\"\n",
    "# model_id = \"granite-code:20b\"\n",
    "\n",
    "granite_via_ollama = find_langchain_model(platform=\"ollama\", model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a18e8d-e828-4acf-854e-350c3cee7f8e",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1a2e2-d222-4308-a572-784ca8b62c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_response = granite_via_ollama.invoke(prompt)\n",
    "\n",
    "print(f\"Granite response from Ollama: {ollama_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558a67-5953-4d22-a03f-5b1bfff4a180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

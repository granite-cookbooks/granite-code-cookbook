{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) with WatsonX, Langchain, and Chroma\n",
    "\n",
    "Based on the IBM Cloud notebook [Use WatsonX, Chroma, and LangChain to answer questions (RAG)](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/d3a5f957-a93b-46cd-82c1-c8d37d4f62c6?context=wx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this notebook\n",
    "This notebook contains instructions for performing Retrieval Augumented Generation (RAG) in watsonx.ai. RAG is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query.\n",
    "\n",
    "RAG use cases include:\n",
    "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
    "- Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
    "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Initial setup:\n",
    "  - Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages using WatsonX, and store them in a vector database.\n",
    "- Upon each user query:\n",
    "  - Retrieve relevant passages from the database. In this recipe, we using an embedding of the query to retrieve semantically similar passages.\n",
    "  - Generate a response by feeding retrieved passage into a large language model, along with the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a [Watson Machine Learning (WML) Service](https://cloud.ibm.com/catalog/services/watson-machine-learning) instance using the free plan. Information about how to create the instance can be found [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the dependencies\n",
    "\n",
    "This notebook was tested with Python 3.12.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"langchain-core==0.2.38\" | tail -n 1\n",
    "!pip install \"langchain==0.2.0\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai==1.1.9\" | tail -n 1\n",
    "!pip install \"langchain_ibm==0.1.12\" | tail -n 1\n",
    "!pip install \"wget==3.2\" | tail -n 1\n",
    "!pip install \"sentence-transformers==3.1.0\" | tail -n 1\n",
    "!pip install \"chromadb==0.5.5\" | tail -n 1\n",
    "!pip install \"pydantic==2.9.1\" | tail -n 1\n",
    "!pip install \"sqlalchemy==2.0.34\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set up the WatsonX API connection credentials\n",
    "Provide the IBM Cloud user API key in an environment variable called `WATSONX_API_KEY`, or at the input. For details, see the <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    apikey = os.environ[\"WATSONX_API_KEY\"]\n",
    "except KeyError:\n",
    "    apikey = input(\"Please enter your apikey (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": apikey,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Provide the project id\n",
    "\n",
    "Provide the WatsonX Project Id in an environment variable called `WATSONX_PROJECT_ID`, or at the input.\n",
    "\n",
    "You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_id = os.environ[\"WATSONX_PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Database\n",
    "\n",
    "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "In this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the document\n",
    "\n",
    "Here we use President Biden's State of the Union address from March 1, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url, out=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "Split the document into text segments that can fit into the model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service. We can check available embedding models using `get_embedding_model_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs\n",
    "\n",
    "get_embedding_model_specs(credentials.get('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id\n",
    "    )\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compatibility watsonx.ai Embeddings with LangChain\n",
    "\n",
    " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WatsonxEmbeddings in module langchain_ibm.embeddings:\n",
      "\n",
      "class WatsonxEmbeddings(pydantic.v1.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
      " |  WatsonxEmbeddings(*, model_id: str = '', project_id: str = '', space_id: str = '', url: Optional[pydantic.v1.types.SecretStr] = None, apikey: Optional[pydantic.v1.types.SecretStr] = None, token: Optional[pydantic.v1.types.SecretStr] = None, password: Optional[pydantic.v1.types.SecretStr] = None, username: Optional[pydantic.v1.types.SecretStr] = None, instance_id: Optional[pydantic.v1.types.SecretStr] = None, version: Optional[pydantic.v1.types.SecretStr] = None, params: Optional[dict] = None, verify: Union[str, bool, NoneType] = None, watsonx_embed: ibm_watsonx_ai.foundation_models.embeddings.embeddings.Embeddings = None, watsonx_client: ibm_watsonx_ai.client.APIClient = None) -> None\n",
      " |\n",
      " |  IBM WatsonX.ai embedding models.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      WatsonxEmbeddings\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.embeddings.embeddings.Embeddings\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  embed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Embed search docs.\n",
      " |\n",
      " |  embed_query(self, text: str) -> List[float]\n",
      " |      Embed query text.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  validate_environment(values: Dict) -> Dict\n",
      " |      Validate that credentials and python package exists in environment.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any from pydantic.v1.json\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  Config = <class 'langchain_ibm.embeddings.WatsonxEmbeddings.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'apikey': typing.Optional[pydantic.v1.types.SecretS...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |\n",
      " |  __custom_root_type__ = False\n",
      " |\n",
      " |  __exclude_fields__ = None\n",
      " |\n",
      " |  __fields__ = {'apikey': ModelField(name='apikey', type=Optional[Secret...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __include_fields__ = None\n",
      " |\n",
      " |  __post_root_validators__ = [(False, <function WatsonxEmbeddings.valida...\n",
      " |\n",
      " |  __pre_root_validators__ = []\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __schema_cache__ = {}\n",
      " |\n",
      " |  __signature__ = <Signature (*, model_id: str = '', project_id: s... ib...\n",
      " |\n",
      " |  __validators__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |\n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |\n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |\n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |\n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |\n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |\n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __get_validators__() -> 'CallableGenerator'\n",
      " |\n",
      " |  __try_update_forward_refs__(**localns: Any) -> None\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |\n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model'\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |\n",
      " |  from_orm(obj: Any) -> 'Model'\n",
      " |\n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model'\n",
      " |\n",
      " |  parse_obj(obj: Any) -> 'Model'\n",
      " |\n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model'\n",
      " |\n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny'\n",
      " |\n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str\n",
      " |\n",
      " |  update_forward_refs(**localns: Any) -> None\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |\n",
      " |  validate(value: Any) -> 'Model'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |\n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.embeddings.embeddings.Embeddings:\n",
      " |\n",
      " |  async aembed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Asynchronous Embed search docs.\n",
      " |\n",
      " |      Args:\n",
      " |          texts: List of text to embed.\n",
      " |\n",
      " |      Returns:\n",
      " |          List of embeddings.\n",
      " |\n",
      " |  async aembed_query(self, text: str) -> List[float]\n",
      " |      Asynchronous Embed query text.\n",
      " |\n",
      " |      Args:\n",
      " |          text: Text to embed.\n",
      " |\n",
      " |      Returns:\n",
      " |          Embedding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WatsonxEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuring the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM watsonx foundation models are among the [LLM models supported by Langchain](https://python.langchain.com/docs/integrations/llms/ibm_watsonx/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select the model\n",
    "Specify the `model_id` of the model that will be used for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_id = ModelTypes.GRANITE_13B_CHAT_V2\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the model parameters\n",
    "Provide the parameters that will be used to configure the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the WatsonX model\n",
    "Using a CustomLLM wrapper from Langchain, initialize the model with defined parameters and the model specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the RAG pipeline\n",
    "\n",
    "Build a question-answering chain with the model and the document retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the question-answering chain to process the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': ' The president said, \"One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\"'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

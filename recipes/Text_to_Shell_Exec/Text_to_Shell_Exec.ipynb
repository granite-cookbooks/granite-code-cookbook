{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Execute a Bash Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe. Here, the generated Bash code is executed as a system command.\n",
    "\n",
    "> **WARNING:** This recipe executes code generated by language models. The generated code may delete or modify files on your system. Use with caution!\n",
    "\n",
    "See the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe for instructions on installing [Ollama](https://ollama.com/) and the models you will need. It also has information on concepts like _system prompts_ and the differences between commands on different operating systems.\n",
    "\n",
    "Change the following cell to match what you used for the model in that notebook. Recall that using a smaller model, like the `3b` default shown here, usually results in generated code that may not be valid for your machine. Use the `20b` model if you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = 'granite-code:3b'  # The `8b` and `20b` models work better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb), we used a Python helper function to determine the host operating system, MacOS, Linux, etc. We used this information in the prompts to encourage the model to generate shell commands that work on the host system, because some shell commands differ between operating systems. However, the output may still contain commands that are not supported by your operating system. Because this notebook attempts to run the generated commands, you may see failures if an incorrect command syntax was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by repeating some of the code we saw in [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb), like `pip` installing the Ollama Python API, if needed, defining some data, like the `examples`, and one of the helper functions, `os_name()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def os_name():\n",
    "    os_name = platform.system()\n",
    "    # It turns out, using \"MacOS\" is better than \"Darwin\", which is what gets returned on MacOS.\n",
    "    # For all other cases, the returned value should be fine as is, so we map the result to the desired\n",
    "    # name, but only for MacOS...\n",
    "    name_map = {'Darwin': 'MacOS'}\n",
    "    shell_map = {'Windows': 'DOS'} # On Windows and use Power Shell, change from `DOS` to `Power Shell`.\n",
    "    # ... then pass the os_name value as the second arg, which is used as the default return value.\n",
    "    # For the shell name, return `bash` by default. (You can change this to zsh, fish, etc.)\n",
    "    return name_map.get(os_name, os_name), shell_map.get(os_name, 'bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_os, my_shell = os_name()\n",
    "my_os, my_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_flags = '-c \"%y %n\" {}'\n",
    "if my_os == 'MacOS':\n",
    "    stat_flags = '-f \"%m %N\" {}'\n",
    "print(f\"The 'stat' flags for my OS \\'{my_os}\\' and \\'{my_shell}\\' are \\'{stat_flags}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Windows system, try changing the \"answers\" in the next cell to be valid Power Shell or DOS commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = f\"\"\"\n",
    "Question:\n",
    "Recursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\n",
    "Answer:\n",
    "find . -name '*.js' | grep -v excludeddir\n",
    "\n",
    "Question:\n",
    "Dump \\\"a0b\\\" as hexadecimal bytes\n",
    "Answer:\n",
    "printf \\\"a0b\\\" | od -tx1\n",
    "\n",
    "Question:\n",
    "create a tar ball of all pdf files in the current folder and any subdirectories.\n",
    "Answer:\n",
    "find . -name '*.pdf' | xargs tar czvf pdf.tar\n",
    "\n",
    "Question:\n",
    "Sort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items\n",
    "Answer:\n",
    "find . -maxdepth 1 -exec stat {stat_flags} \\; | sort -n -r | tail -n 7\n",
    "\n",
    "Question:\n",
    "find all the empty directories in and under the current directory.\n",
    "Answer:\n",
    "find . -type d -empty\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Prompts for Direct Execution\n",
    "\n",
    "Rather than work through the three successive ways to invoke generation using Ollama, we will just use the `chat()` helper function we used in the previous notebook.\n",
    "\n",
    "But first, we want to modify the _system prompt_ we used in the previous notebook to work better for our purposes.\n",
    "\n",
    "Writing prompts is an art. Recall in [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb), our output was usually Markdown with quoted sections of shell code and commentary explaining how it worked. Here, we just want code output that we execute without editing. Here are a few tips on writing prompts for our purposes here:\n",
    "\n",
    "> **TIPS:**\n",
    ">\n",
    "> 1. Rather than use a question (\"How do I ...?\") in a prompt, provide a directive (\"Write a script that ...\"). This helps prevent the model from generating dialogue around the code.\n",
    "> 2. Add instructions to the system prompt like this: \"You are a helpful software engineer. You write clear, concise, well-commented code. You only print valid code. You don't print any commentary about the code nor markdown syntax to wrap the code.\"\n",
    "\n",
    "So here is our new system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_system_prompt = f\"\"\"\n",
    "    You are a helpful software engineer. You write clear, concise,\n",
    "    well-commented code. You only print valid code. You don't print\n",
    "    any commentary about the code nor do you wrap the code in markdown syntax!\n",
    "    You make sure you only generate {my_shell} code that is {my_os}-compatible!\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same `chat()` helper from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt: str,\n",
    "         system_prompt:str = default_system_prompt,\n",
    "         examples: str = examples,\n",
    "         model: str = default_model) -> str:\n",
    "    user_prompt = f\"\"\"\n",
    "        {examples}\n",
    "        Question:\n",
    "        {prompt}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role':'system',\n",
    "        'content': system_prompt\n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt\n",
    "      },\n",
    "    ])\n",
    "\n",
    "    result = response['message']['content']\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_code1 = chat(f\"\"\"\n",
    "    Write a {my_shell} script to print the first 50 files found under the current working directory\n",
    "    that have been modified within the last week. Make sure you show the last modification time\n",
    "    for each file in the output.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this output to what you got in the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe. Is this output a valid script and nothing else? Or, is there extra commentary and Markdown formatting? If you got this extra, undesirable output, try running the cell again. Does modifying the prompt or system prompt help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can attempt to execute the script! There is no need for an additional helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system(shell_code1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_code2 = chat(f\"\"\"Write a {my_shell} script to recursively find Jupyter notebooks\n",
    "in the parent directory and print their paths.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(shell_code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try invoking `chat()` several times. How do the responses change from one invocation to the next? Try different queries. adding more examples to the `examples` string or modifying the ones shown. Does this affect the outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

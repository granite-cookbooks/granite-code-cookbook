{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generate a Retrieval Augmented Generation (RAG) Application Code using IBM Granite Models</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Notebook goals:</h3>\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "1. Connect to ibm-granite/granite-8b-code-instruct-128k hosted on Replicate, and use it to generate a sample RAG Application code\n",
    "2. Connect to ibm-granite/granite-20b-code-instruct-8k hosted on Replicate, and use it to generate a sample RAG Application code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prerequisites: </h3>\n",
    "\n",
    "1. Create an account on Replicate. \n",
    "2. Copy the Replicate API Token to an environment file (.env file) and place it in the same directory as this notebook. Environment variable can be named as `REPLICATE_API_TOKEN`. \n",
    "3. Install `langchain` and `ibm-granite-community/utils` python packages using below pip command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the variables from .env file</h3>\n",
    "\n",
    "The code below imports necessary packages, and loads the environment variable `REPLICATE_API_TOKEN` from the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ibm_granite_community.langchain_utils import find_langchain_model\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API token directly from environment variables\n",
    "replicate_api_token = os.getenv('REPLICATE_API_TOKEN')\n",
    "\n",
    "if replicate_api_token:\n",
    "    print('API token loaded successfully')\n",
    "else:\n",
    "    print('Failed to load API token. Please check your .env file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Selection </h3>\n",
    "\n",
    "In this section, we specify the model ID used to invoke specific IBM Granite Models hosted on Replicate platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model : granite-8b-code-instruct-128k</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "# model_id = \"ibm-granite/granite-20b-code-instruct-8k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code snippets use the ibm-granite/granite-8b-code-instruct-128k model to generate a sample code for RAG Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Retrieval </h3>\n",
    "\n",
    "Here, we retrieve the model using the find_langchain_model function. This function is designed to locate and initialize models hosted on Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model via replicate platform\n",
    "granite_via_replicate = find_langchain_model(platform=\"Replicate\", model_id=model_id,\n",
    "                                             model_kwargs={\"top_k\": 50,\"temperature\":0.0,\"top_p\": 1,\"max_tokens\":4096})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Parameters\n",
    "\n",
    "    top_k: Samples tokens with the highest probabilities until the specified number of tokens is reached. Integer in the range 1 to 100. Default Value = 50. Higher values lead to greater variability. \n",
    "\n",
    "    temperature: Flattens or sharpens the probability distribution over the tokens to be sampled. Floating-point number in the range 0.0 (same as greedy decoding) to 2.0 (maximum creativity). Default Value = 0.7. Higher values lead to greater variability.\n",
    "    \n",
    "    top_p: Samples tokens with the highest probability scores until the sum of the scores reaches the specified threshold value. Floating-point number in the range 0.0 to 1.0. Default Value = 1.0.\n",
    "    \n",
    "    max_tokens: Control the length of the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `zeroshot_prompt` generates a prompt to create a sample RAG Code based on a natural language question without prior examples (zero-shot prompting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_prompt(question):\n",
    "    prompt = f\"\"\"You are are a Generative AI expert with 20 years of experience writing complex RAG Code. Your task is to write good quality Generative AI code and nothing else. \n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_answer_using_zeroshot` generates the result from ibm-granite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_using_zeroshot(question):\n",
    "    prompt = zeroshot_prompt(question)\n",
    "    return granite_via_replicate.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Testing with multiple prompts</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain Retrieval Augmented Generation Technique with a sample code.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model : granite-20b-code-instruct-8k</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "model_id = \"ibm-granite/granite-20b-code-instruct-8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model via replicate platform\n",
    "granite_via_replicate = find_langchain_model(platform=\"Replicate\", model_id=model_id,\n",
    "                                             model_kwargs={\"top_k\": 50,\"temperature\":0.0,\"top_p\": 1,\"max_tokens\":4096})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code snippets use the ibm-granite/granite-20b-code-instruct-8k model to generate a sample code for RAG Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain Retrieval Augmented Generation Technique with a sample code.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique. Use a Vector Database to store a sample corpus. Finally, use a code model to retrieve correct answers.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

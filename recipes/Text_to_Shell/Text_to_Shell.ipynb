{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generating Bash Code with Granite Code and Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite: Install Ollama and Granite Code models\n",
    "\n",
    "1. [Download and Start Ollama](https://ollama.com/download)\n",
    "1. Install Granite Code 20b: `ollama pull granite-code:20b`\n",
    "1. Install Granite Code 8b: `ollama pull granite-code:8b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## One-shot Prompt with Granite Code 20b\n",
    "\n",
    "In One-shot prompting, you provide the model with a question and no examples. The model will generate an answer given its training. Larger models tend to do better at this task.\n",
    "\n",
    "Use the [ollama-python package](https://github.com/ollama/ollama-python) to access the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "  model='granite-code:20b',\n",
    "  prompt=\"How can I use a bash script to calculate the number of days between the dates Jan 12, 2023 and Jan 20, 2024?\"\n",
    ")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running on a Mac, it is likely that the invocation of `date` will be incorrect for the MacOS version of this command. Try adding the following sentence to the prompt and see what happens: `Make sure the generated shell commands are MacOS-compatible.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting with Granite Code 8b\n",
    "\n",
    "In few-shot prompting, you provide the model with a question and some examples. The model will generate an answer given its training. The additional examples help the model zero in on a pattern, which may be required for smaller models to perform well at this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Question:\n",
    "Recursively finds files like '*.js', and filters out files with 'excludeddir' in path.\n",
    "Answer:\n",
    "find . -name '*.js' | grep -v excludeddir\n",
    "\n",
    "Question:\n",
    "Dump \\\"a0b\\\" as hexadecimal bytes\n",
    "Answer:\n",
    "printf \\\"a0b\\\" | od -tx1\n",
    "\n",
    "Question:\n",
    "create a tar ball of all pdf files in current folder\n",
    "Answer:\n",
    "find . -name *.pdf | xargs tar czvf /root/Desktop/evidence/pdf.tar\n",
    "\n",
    "Question:\n",
    "Sort all files/directories under current directory according to modification time and print only the recent 7 of them\n",
    "Answer:\n",
    "find -mindepth 1 -printf \\\"%T@ %Pn\\\" | sort -n -r | cut -d' ' -f 2- | tail -n +7\n",
    "\n",
    "Question:\n",
    "find all the empty directories in the current folder\n",
    "Answer:\n",
    "find . -type d -empty\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query = \"How can I use a bash script to calculate the number of days between the dates Jan 12, 2023 and Jan 20, 2024?\"\n",
    "\n",
    "user_prompt = examples + \"\\nQuestion:\\n\" + query + \"\\nAnswer:\\n\"\n",
    "\n",
    "response = ollama.chat(model='granite-code:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': user_prompt\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a System Prompt\n",
    "\n",
    "A system prompt can be used to provide additional instructions and clarity or context for the task. Here we let the model know what we expect from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful software engineer. You write clear, concise, well-commented code.\"\n",
    "\n",
    "response = ollama.chat(model='granite-code:8b', messages=[\n",
    "  {\n",
    "    'role':'system',\n",
    "    'content': system_prompt  \n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': user_prompt\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generating Bash Code with Granite Code and Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** This recipe assumes you are working on a Linux, MacOS, or other UNIX-compatible system. While we haven't tested on Windows, some of the examples may generate valid DOS or PowerShell output.  \n",
    "\n",
    "### Prerequisite: Install Ollama and Granite Code models\n",
    "\n",
    "1. [Download and Start Ollama](https://ollama.com/download)\n",
    "1. Install Granite Code 20b: `ollama pull granite-code:20b`\n",
    "1. Install Granite Code 8b: `ollama pull granite-code:8b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## One-shot Prompt with Granite Code 20b\n",
    "\n",
    "In One-shot prompting, you provide the model with a question and no examples. The model will generate an answer given its training. Larger models tend to do better at this task.\n",
    "\n",
    "Use the [ollama-python package](https://github.com/ollama/ollama-python) to access the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write two helper functions that we'll use for all our queries. First, we'll find it useful to determine the name of our operating system and use that string in queries. This is because shell commands sometimes have different options on Linux vs. MacOS, etc. We'll write our queries so they take this difference into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TIP:** If you are using MacOS, you can install Linux-compatible versions of many commands. Consider these two options: \n",
    "> \n",
    "> * Install GNU Coreutils on a Mac. See [these instructions](https://superuser.com/questions/476575/replace-os-xs-shell-commands-with-the-linux-versions).\n",
    "> * Install [HomeBrew](https://brew.sh/) and use it to install Linux-compatible (and other) tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "def os_name():\n",
    "    os_name = platform.system()\n",
    "    # It turns out, using \"MacOS\" is better than \"Darwin\", which is what gets returned on MacOS. \n",
    "    # For all other cases, the returned value should be fine as is, so we map the result to the desired\n",
    "    # name, but only for MacOS...\n",
    "    name_map = {'Darwin': 'MacOS'}\n",
    "    # ... then pass the os_name value as the second arg, which is used as the default return value.\n",
    "    return name_map.get(os_name, os_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"My OS is {os_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a helper function for running queries, wrapping the Ollama `generate()` API call. The user specifies the prompt and a model name, but we define the default model name for Granite Code 20B, `granite-code:20b`. Also, note how we add additional context to the user's input prompt, such as _\"make sure you write code that works for _my_ system!\"_ (We'll see another way to do this below.)\n",
    "\n",
    "The reason we print the result, then return it, is to get the best output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt: str, model: str = 'granite-code:20b') -> str:\n",
    "    response = ollama.generate(\n",
    "        model=model, \n",
    "        prompt=f\"{prompt}. Make sure you generate code that is {os_name()}-compatible!\")\n",
    "    result = response[\"response\"]\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = query(\"\"\"\n",
    "    Show me a bash script to print the first 50 files found under the current working directory\n",
    "    that have been modified within the last week. Make sure you show the last modification time \n",
    "    for each file in the output.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the command in the next cell. _**Keep the `!` shown**_, e.g., `!ls -al`, so Jupyter knows to run the command as a shell script instead of Python. (You can omit lines like `#!/bin/bash`.) \n",
    "\n",
    "Does the script work? If not try running the query again. Also try modifying the query string. What difference do these steps make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore execution of generated shell code in the next recipe we recommend you study after this one, [../Text_to_Shell_Exec](../Text_to_Shell_Exec/Text_to_Shell_Exec.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Prompting with Granite Code 8b\n",
    "\n",
    "In few-shot prompting, you provide the model with a question and some examples. The model will generate an answer given its training. The additional examples help the model zero in on a pattern, which may be required for smaller models to perform well at this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Question:\n",
    "Recursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\n",
    "Answer:\n",
    "find . -name '*.js' | grep -v excludeddir\n",
    "\n",
    "Question:\n",
    "Dump \\\"a0b\\\" as hexadecimal bytes\n",
    "Answer:\n",
    "printf \\\"a0b\\\" | od -tx1\n",
    "\n",
    "Question:\n",
    "create a tar ball of all pdf files in the current folder and any subdirectories.\n",
    "Answer:\n",
    "find . -name '*.pdf' | xargs tar czvf pdf.tar\n",
    "\n",
    "Question:\n",
    "Sort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items\n",
    "Answer:\n",
    "find . -maxdepth 1 -exec stat -f \"%Sm {}\" \\; | sort -n -r | tail -n 7\n",
    "\n",
    "Question:\n",
    "find all the empty directories in and under the current directory.\n",
    "Answer:\n",
    "find . -type d -empty\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define another helper function for calling `ollama.chat()`. Why it is called `chat1()` will be explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat1(prompt: str, examples: str = examples, model: str ='granite-code:8b') -> str:\n",
    "    user_prompt = f\"\"\"\n",
    "        {examples}\n",
    "        Question:\n",
    "        {prompt}. Make sure you generate code that is {os_name()}-compatible!\n",
    "        Answer:\"\"\"\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt\n",
    "      },\n",
    "    ])\n",
    "    result = response['message']['content']\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chat1(\"\"\"\n",
    "    Show me a bash script to print the first 50 files found under the current working directory\n",
    "    that have been modified within the last week. Make sure you show the last modification time \n",
    "    for each file in the output.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a System Prompt\n",
    "\n",
    "Finally, a _system prompt_ is the preferred way to provide additional instructions and clarity about the context for a task, especially when this same information applies for _all_ user queries in the application. When you are building an AI-enabled application for a set of use cases, you will probably spend a lot of time refining the system prompt to maximize the quality of the results!\n",
    "\n",
    "Here we define a `default_system_prompt` to let the model know what we expect from it.\n",
    "\n",
    "So, let's define a final helper function, `chat()`, that includes a system prompt, where `default_system_prompt` is the default. Also, note that we move the sentence `Make sure you only generate code that is {os_name()}-compatible!` to the system prompt, where it really belongs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_system_prompt = f\"\"\"\n",
    "    You are a helpful software engineer. You write clear, concise, \n",
    "    well-commented code. Make sure you only generate code that is \n",
    "    {os_name()}-compatible!\n",
    "    \"\"\"\n",
    "\n",
    "def chat(prompt: str, \n",
    "         system_prompt:str = default_system_prompt,\n",
    "         examples: str = examples, \n",
    "         model: str ='granite-code:8b') -> str:\n",
    "    user_prompt = f\"\"\"\n",
    "        {examples}\n",
    "        Question:\n",
    "        {prompt}\n",
    "        Answer:\"\"\" \n",
    "    response = ollama.chat(model=model, messages=[\n",
    "      {\n",
    "        'role':'system',\n",
    "        'content': system_prompt  \n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt\n",
    "      },\n",
    "    ])\n",
    "    result = response['message']['content']\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = chat(\"\"\"\n",
    "    Show me a bash script to print the first 50 files found under the current working directory\n",
    "    that have been modified within the last week. Make sure you show the last modification time \n",
    "    for each file in the output.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify `chat()` to return the whole `response`, what additional information do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
